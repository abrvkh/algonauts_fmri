{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import things\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import gpsm\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import Dinov2Config, Dinov2Model, AutoImageProcessor\n",
    "PATH = '/home/anastasia/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(): \n",
    "    try: \n",
    "        del fmri_model\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        del embed_model\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        del image_processor\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        del d\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        del y_l\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        del y_r\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        del features\n",
    "    except NameError:\n",
    "        pass\n",
    "    try: \n",
    "        del out_lh\n",
    "    except NameError:\n",
    "        pass \n",
    "    try:\n",
    "        del out_rh\n",
    "    except NameError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH training fMRI data shape:\n",
      "(9841, 19004)\n",
      "(Training stimulus images × LH vertices)\n",
      "\n",
      "RH training fMRI data shape:\n",
      "(9841, 20544)\n",
      "(Training stimulus images × RH vertices)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# load fmri\n",
    "subj = 'subj01'\n",
    "data_dir = '/home/anastasia/datasets/fmri/subjects/' + subj\n",
    "fmri_dir = os.path.join(data_dir, 'training_split', 'training_fmri')\n",
    "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "\n",
    "print('LH training fMRI data shape:')\n",
    "print(lh_fmri.shape)\n",
    "print('(Training stimulus images × LH vertices)')\n",
    "\n",
    "print('\\nRH training fMRI data shape:')\n",
    "print(rh_fmri.shape)\n",
    "print('(Training stimulus images × RH vertices)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_roi_fmri(data_dir, fmri_full, hem, roi):\n",
    "  # Define the ROI class based on the selected ROI\n",
    "  if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "    roi_class = 'prf-visualrois'\n",
    "  elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "      roi_class = 'floc-bodies'\n",
    "  elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "      roi_class = 'floc-faces'\n",
    "  elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "      roi_class = 'floc-places'\n",
    "  elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "      roi_class = 'floc-words'\n",
    "  elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "      roi_class = 'streams'\n",
    "\n",
    "  # Load the ROI brain surface maps\n",
    "  challenge_roi_class_dir = os.path.join(data_dir, 'roi_masks',\n",
    "      hem[0]+'h.'+roi_class+'_challenge_space.npy')\n",
    "  roi_map_dir = os.path.join(data_dir, 'roi_masks',\n",
    "      'mapping_'+roi_class+'.npy')\n",
    "  challenge_roi_class = np.load(challenge_roi_class_dir)\n",
    "  roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
    "\n",
    "  # Select the vertices corresponding to the ROI of interest\n",
    "  roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
    "  challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int)\n",
    "  # Get the subset of fmri data\n",
    "  idx_roi = np.where(challenge_roi)[0]\n",
    "  the_fmri = fmri_full[:,idx_roi]\n",
    "\n",
    "  mapping_size = the_fmri.shape[1] # number of vertices in this roi\n",
    "\n",
    "  return the_fmri, mapping_size, idx_roi, challenge_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n",
      "Training image file name: train-0001_nsd-00013.png\n",
      "73k NSD images ID: 00013\n"
     ]
    }
   ],
   "source": [
    "# load images \n",
    "train_img_dir  = os.path.join(data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "train_img_file = train_img_list[0]\n",
    "print('Training image file name: ' + train_img_file)\n",
    "print('73k NSD images ID: ' + train_img_file[-9:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stimulus images: 8857\n",
      "Validation stimulus images: 984\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "# create train, validation and test partitions\n",
    "rand_seed = 5 \n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Calculate how many stimulus images correspond to 90% of the training data\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "# Shuffle all training stimulus images\n",
    "idxs = np.arange(len(train_img_list))\n",
    "np.random.shuffle(idxs)\n",
    "# Assign 90% of the shuffled stimulus images to the training partition,\n",
    "# and 10% to the test partition\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "# No need to shuffle or split the test stimulus images\n",
    "idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('Validation stimulus images: ' + format(len(idxs_val)))\n",
    "print('Test stimulus images: ' + format(len(idxs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), transforms.Resize(244), transforms.CenterCrop(224), transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "#transform = AutoImageProcessor.from_pretrained('facebook/dinov2-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_paths, idxs, transform, lh_fmri, rh_fmri):\n",
    "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "        self.transform = transform\n",
    "        self.lh_fmri = lh_fmri[idxs] \n",
    "        self.rh_fmri = rh_fmri[idxs]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.imgs_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "        if self.transform:\n",
    "            img = self.transform(img).to(device)\n",
    "        _lh_fmri = torch.tensor(self.lh_fmri[idx]).to(device)\n",
    "        _rh_fmri = torch.tensor(self.rh_fmri[idx]).to(device)\n",
    "        return img, _lh_fmri, _rh_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 \n",
    "\n",
    "# Get the paths of all image files\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "# The DataLoaders contain the ImageDataset class\n",
    "train_imgs_dataloader = DataLoader(\n",
    "      ImageDataset(train_imgs_paths, idxs_train, transform, lh_fmri, rh_fmri), \n",
    "      batch_size=batch_size\n",
    "  )\n",
    "val_imgs_dataloader = DataLoader(\n",
    "      ImageDataset(train_imgs_paths, idxs_val, transform, lh_fmri, rh_fmri), \n",
    "      batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n",
      "Training image file name: train-0001_nsd-00013.png\n",
      "73k NSD images ID: 00013\n"
     ]
    }
   ],
   "source": [
    "# load images \n",
    "train_img_dir  = os.path.join(data_dir, 'training_split', 'training_images')\n",
    "test_img_dir  = os.path.join(data_dir, 'test_split', 'test_images')\n",
    "\n",
    "# Create lists will all training and test image file names, sorted\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "test_img_list = os.listdir(test_img_dir)\n",
    "test_img_list.sort()\n",
    "print('Training images: ' + str(len(train_img_list)))\n",
    "print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "train_img_file = train_img_list[0]\n",
    "print('Training image file name: ' + train_img_file)\n",
    "print('73k NSD images ID: ' + train_img_file[-9:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2Model(\n",
       "  (embeddings): Dinov2Embeddings(\n",
       "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): Dinov2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x Dinov2Layer(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): Dinov2Attention(\n",
       "          (attention): Dinov2SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): Dinov2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_scale1): Dinov2LayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Dinov2MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_scale2): Dinov2LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model for feature embedding\n",
    "embed_model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "embed_model.to(device)\n",
    "embed_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model part\n",
    "\n",
    "Here we test the linear regression which we consider as a baseline for predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1108/1108 [20:29<00:00,  1.11s/it]\n",
      "100%|██████████| 123/123 [01:59<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation is 0.45105930244016057\n"
     ]
    }
   ],
   "source": [
    "# regression test \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "the_fmri, mapping_size, idx_roi, mask = select_roi_fmri(data_dir, lh_fmri, 'lh', \"V1v\")\n",
    "\n",
    "ss_layers = [12]\n",
    "\n",
    "features_train = []\n",
    "y_l_train = []\n",
    "y_r_train = []\n",
    "for it, (d,y_l,y_r) in tqdm(enumerate(train_imgs_dataloader), total=len(train_imgs_dataloader)):\n",
    "    d, y_l, y_r = d.to(device), y_l, y_r\n",
    "    # pass data through DinoV2\n",
    "    with torch.no_grad():\n",
    "        #features = embed_model(d, output_hidden_states=True).hidden_states\n",
    "        features = embed_model(d).last_hidden_state\n",
    "    # select subset of layers from embed_model \n",
    "    # features = torch.cat([features[i].unsqueeze(1) for i in ss_layers], dim=1) # [8, 1, 257, 768]\n",
    "    features = features.flatten(1) # [8, 257 * 768]\n",
    "    # features = features[:,0,:]\n",
    "    features_train += [features] \n",
    "    y_l_train += [y_l[:,idx_roi]]\n",
    "    y_r_train += [y_r[:,idx_roi]]\n",
    "\n",
    "features_train = torch.cat(features_train, dim=0)\n",
    "y_l_train = torch.cat(y_l_train, dim=0)\n",
    "y_r_train = torch.cat(y_r_train, dim=0)\n",
    "reg_fit = LinearRegression().fit(features_train.cpu().numpy(), y_l_train.cpu().numpy())\n",
    "\n",
    "del features_train\n",
    "del y_l_train\n",
    "del y_r_train\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "features_val = []\n",
    "y_l_val = []\n",
    "y_r_val = []\n",
    "for it, (d,y_l,y_r) in tqdm(enumerate(val_imgs_dataloader), total=len(val_imgs_dataloader)):\n",
    "    d, y_l, y_r = d.to(device), y_l, y_r\n",
    "    # pass data through DinoV2\n",
    "    with torch.no_grad():\n",
    "        # features = embed_model(d, output_hidden_states=True).hidden_states\n",
    "        features = embed_model(d).last_hidden_state\n",
    "    # select subset of layers from embed_model \n",
    "    # features = torch.cat([features[i].unsqueeze(1) for i in ss_layers], dim=1)\n",
    "    features = features.flatten(1) # [8, 257 * 768]\n",
    "    # features = features[:,0,:]\n",
    "    features_val += [features]\n",
    "    y_l_val += [y_l[:,idx_roi]]\n",
    "    y_r_val += [y_r[:,idx_roi]]\n",
    "\n",
    "features_val = torch.cat(features_val, dim=0)\n",
    "y_l_val = torch.cat(y_l_val, dim=0).cpu().numpy()\n",
    "y_r_val = torch.cat(y_r_val, dim=0).cpu().numpy()\n",
    "val_preds = reg_fit.predict(features_val.cpu().numpy())\n",
    "\n",
    "del features_val\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Empty correlation array of shape: (LH vertices)\n",
    "correlation = np.zeros(val_preds.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in range(val_preds.shape[1]):\n",
    "    correlation[v] = scipy.stats.pearsonr(val_preds[:,v],y_l_val[:,v])[0]\n",
    "print('\\nCorrelation is '+ str(np.mean(correlation)))\n",
    "\n",
    "del y_l_val\n",
    "del y_r_val\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
